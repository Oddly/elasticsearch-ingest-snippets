# Enhanced GitHub Actions workflow to validate Elasticsearch ingest pipelines
name: Validate Ingest Pipelines

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  validate:
    runs-on: ubuntu-latest
    
    services:
      elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
        ports:
          - 9200:9200
        env:
          discovery.type: single-node
          xpack.security.enabled: "false"
          ES_JAVA_OPTS: "-Xms512m -Xmx512m"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          # Fetch more history for better diff detection
          fetch-depth: 0

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip3 install requests

      - name: Wait for Elasticsearch and run validation
        run: |
          cat > validate_pipelines.py << 'PYTHON_SCRIPT_EOF'
          #!/usr/bin/env python3
          import json
          import os
          import re
          import requests
          import sys
          import time
          from pathlib import Path

          def wait_for_elasticsearch(elasticsearch_url="http://localhost:9200", timeout=120):
              """Wait for Elasticsearch to be ready and healthy"""
              print("🔍 Waiting for Elasticsearch to start...")
              
              start_time = time.time()
              while time.time() - start_time < timeout:
                  try:
                      # Check if Elasticsearch is responding
                      response = requests.get(elasticsearch_url, timeout=5)
                      if response.status_code == 200:
                          print("✅ Elasticsearch is responding!")
                          break
                  except requests.exceptions.RequestException:
                      pass
                  
                  print(".", end="", flush=True)
                  time.sleep(2)
              else:
                  raise Exception(f"Elasticsearch did not start within {timeout} seconds")
              
              # Wait for cluster health
              print("🔍 Waiting for Elasticsearch cluster to be healthy...")
              start_time = time.time()
              while time.time() - start_time < 60:
                  try:
                      health_response = requests.get(
                          f"{elasticsearch_url}/_cluster/health?wait_for_status=yellow&timeout=10s",
                          timeout=15
                      )
                      if health_response.status_code == 200:
                          health_data = health_response.json()
                          if health_data.get('status') in ['green', 'yellow']:
                              print("✅ Elasticsearch cluster is healthy!")
                              print(f"   Cluster status: {health_data.get('status')}")
                              return
                  except requests.exceptions.RequestException:
                      pass
                  
                  print(".", end="", flush=True)
                  time.sleep(2)
              else:
                  print("⚠️  Warning: Cluster health check timed out, proceeding anyway")

          def fix_triple_quotes(content):
              """Convert triple-quoted strings to properly escaped JSON strings"""
              def replace_triple_quote(match):
                  inner_content = match.group(1)
                  # Use json.dumps to properly escape, then remove the outer quotes
                  escaped = json.dumps(inner_content)[1:-1]
                  return '"' + escaped + '"'
              
              pattern = r'"""(.*?)"""'
              return re.sub(pattern, replace_triple_quote, content, flags=re.DOTALL)

          def load_and_fix_pipeline(pipeline_file):
              """Load a pipeline file and fix any triple-quote issues"""
              try:
                  with open(pipeline_file, 'r') as f:
                      content = f.read()
                  
                  # Fix triple quotes if present
                  fixed_content = fix_triple_quotes(content)
                  
                  # Parse and validate JSON
                  return json.loads(fixed_content)
              except Exception as e:
                  raise Exception(f"Failed to load pipeline from {pipeline_file}: {e}")

          def load_json_file(filepath):
              """Load and validate a JSON file"""
              try:
                  with open(filepath, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  raise Exception(f"Failed to load JSON from {filepath}: {e}")

          def normalize_result(result):
              """Remove timestamp fields for comparison"""
              if isinstance(result, dict):
                  return {k: normalize_result(v) for k, v in result.items() if k != 'timestamp'}
              elif isinstance(result, list):
                  return [normalize_result(item) for item in result]
              else:
                  return result

          def simulate_pipeline(pipeline_data, docs_data, elasticsearch_url="http://localhost:9200"):
              """Send pipeline simulation request to Elasticsearch"""
              payload = {
                  "pipeline": pipeline_data,
                  "docs": docs_data
              }
              
              try:
                  response = requests.post(
                      f"{elasticsearch_url}/_ingest/pipeline/_simulate",
                      json=payload,
                      headers={"Content-Type": "application/json"},
                      timeout=30
                  )
                  response.raise_for_status()
                  return response.json()
              except Exception as e:
                  raise Exception(f"Elasticsearch simulation failed: {e}")

          def get_pipelines_to_test():
              """Determine which pipelines to test based on Git changes"""
              pipelines_to_test = os.environ.get('PIPELINES_TO_TEST', '').strip()
              test_count = int(os.environ.get('TEST_COUNT', '0'))
              
              if test_count == 0:
                  print("✅ No pipelines to test - skipping validation.")
                  return []
              
              if pipelines_to_test:
                  return pipelines_to_test.split()
              else:
                  # Fallback to finding all pipeline directories
                  pipeline_dirs = []
                  pipelines_path = Path('pipelines')
                  if pipelines_path.exists():
                      for item in pipelines_path.iterdir():
                          if item.is_dir():
                              pipeline_dirs.append(str(item))
                  return pipeline_dirs

          def main():
              # Wait for Elasticsearch to be ready
              wait_for_elasticsearch()
              
              print(f"🧪 Starting pipeline validation...")
              
              pipelines = get_pipelines_to_test()
              if not pipelines:
                  return 0
              
              print(f"Testing {len(pipelines)} pipeline(s)...")
              print()
              
              tests_run = 0
              tests_passed = 0
              tests_failed = 0
              
              for pipeline_dir in pipelines:
                  pipeline_path = Path(pipeline_dir)
                  pipeline_file = pipeline_path / "pipeline.json"
                  example_file = pipeline_path / "simulate_example.json"
                  results_file = pipeline_path / "simulate_results.json"
                  
                  # Check if all required files exist
                  if not all(f.exists() for f in [pipeline_file, example_file, results_file]):
                      print("---")
                      print(f"ℹ️  [INFO] Skipping directory {pipeline_dir} (missing required .json files)")
                      continue
                  
                  print("---")
                  print(f"🧪 [TEST] Testing pipeline in directory: {pipeline_dir}")
                  tests_run += 1
                  
                  try:
                      # Load pipeline and example data
                      pipeline_data = load_and_fix_pipeline(pipeline_file)
                      example_data = load_json_file(example_file)
                      expected_result = load_json_file(results_file)
                      
                      # Run simulation
                      actual_result = simulate_pipeline(pipeline_data, example_data['docs'])
                      
                      # Normalize results for comparison (remove timestamps)
                      normalized_actual = normalize_result(actual_result)
                      normalized_expected = normalize_result(expected_result)
                      
                      # Compare results
                      if normalized_actual == normalized_expected:
                          print("✅ [SUCCESS] Simulation result matches expected result.")
                          tests_passed += 1
                      else:
                          print("❌ [FAILURE] Simulation result does not match expected result.")
                          print("---------- EXPECTED ----------")
                          print(json.dumps(normalized_expected, indent=2, sort_keys=True))
                          print("----------- ACTUAL -----------")
                          print(json.dumps(normalized_actual, indent=2, sort_keys=True))
                          print("----------------------------")
                          tests_failed += 1
                  
                  except Exception as e:
                      print(f"❌ [ERROR] {e}")
                      tests_failed += 1
              
              print()
              print("📊 Test Summary:")
              print(f"   Tests run: {tests_run}")
              print(f"   Passed: {tests_passed}")
              print(f"   Failed: {tests_failed}")
              print()
              
              if tests_failed > 0:
                  print("❌ One or more pipeline tests failed.")
                  return 1
              else:
                  print("✅ All pipeline tests passed!")
                  return 0

          if __name__ == "__main__":
              sys.exit(main())
          PYTHON_SCRIPT_EOF
          
          # Set environment variables from previous step
          export PIPELINES_TO_TEST="${{ steps.changed-pipelines.outputs.pipelines_to_test }}"
          export TEST_COUNT="${{ steps.changed-pipelines.outputs.test_count }}"
          
          # Run the Python validation script
          python3 validate_pipelines.py
        run: |
          cat > validate_pipelines.py << 'PYTHON_SCRIPT_EOF'
          #!/usr/bin/env python3
          import json
          import os
          import re
          import requests
          import sys
          from pathlib import Path
          import time

          def fix_triple_quotes(content):
              """Convert triple-quoted strings to properly escaped JSON strings"""
              def replace_triple_quote(match):
                  inner_content = match.group(1)
                  # Use json.dumps to properly escape, then remove the outer quotes
                  escaped = json.dumps(inner_content)[1:-1]
                  return '"' + escaped + '"'
              
              pattern = r'"""(.*?)"""'
              return re.sub(pattern, replace_triple_quote, content, flags=re.DOTALL)

          def load_and_fix_pipeline(pipeline_file):
              """Load a pipeline file and fix any triple-quote issues"""
              try:
                  with open(pipeline_file, 'r') as f:
                      content = f.read()
                  
                  # Fix triple quotes if present
                  fixed_content = fix_triple_quotes(content)
                  
                  # Parse and validate JSON
                  return json.loads(fixed_content)
              except Exception as e:
                  raise Exception(f"Failed to load pipeline from {pipeline_file}: {e}")

          def load_json_file(filepath):
              """Load and validate a JSON file"""
              try:
                  with open(filepath, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  raise Exception(f"Failed to load JSON from {filepath}: {e}")

          def normalize_result(result):
              """Remove timestamp fields for comparison"""
              if isinstance(result, dict):
                  return {k: normalize_result(v) for k, v in result.items() if k != 'timestamp'}
              elif isinstance(result, list):
                  return [normalize_result(item) for item in result]
              else:
                  return result

          def simulate_pipeline(pipeline_data, docs_data, elasticsearch_url="http://localhost:9200"):
              """Send pipeline simulation request to Elasticsearch"""
              payload = {
                  "pipeline": pipeline_data,
                  "docs": docs_data
              }
              
              try:
                  response = requests.post(
                      f"{elasticsearch_url}/_ingest/pipeline/_simulate",
                      json=payload,
                      headers={"Content-Type": "application/json"},
                      timeout=30
                  )
                  response.raise_for_status()
                  return response.json()
              except Exception as e:
                  raise Exception(f"Elasticsearch simulation failed: {e}")

          def get_pipelines_to_test():
              """Determine which pipelines to test based on Git changes"""
              pipelines_to_test = os.environ.get('PIPELINES_TO_TEST', '').strip()
              test_count = int(os.environ.get('TEST_COUNT', '0'))
              
              if test_count == 0:
                  print("✅ No pipelines to test - skipping validation.")
                  return []
              
              if pipelines_to_test:
                  return pipelines_to_test.split()
              else:
                  # Fallback to finding all pipeline directories
                  pipeline_dirs = []
                  pipelines_path = Path('pipelines')
                  if pipelines_path.exists():
                      for item in pipelines_path.iterdir():
                          if item.is_dir():
                              pipeline_dirs.append(str(item))
                  return pipeline_dirs

          def main():
              print(f"🧪 Starting pipeline validation...")
              
              pipelines = get_pipelines_to_test()
              if not pipelines:
                  return 0
              
              print(f"Testing {len(pipelines)} pipeline(s)...")
              print()
              
              tests_run = 0
              tests_passed = 0
              tests_failed = 0
              
              for pipeline_dir in pipelines:
                  pipeline_path = Path(pipeline_dir)
                  pipeline_file = pipeline_path / "pipeline.json"
                  example_file = pipeline_path / "simulate_example.json"
                  results_file = pipeline_path / "simulate_results.json"
                  
                  # Check if all required files exist
                  if not all(f.exists() for f in [pipeline_file, example_file, results_file]):
                      print("---")
                      print(f"ℹ️  [INFO] Skipping directory {pipeline_dir} (missing required .json files)")
                      continue
                  
                  print("---")
                  print(f"🧪 [TEST] Testing pipeline in directory: {pipeline_dir}")
                  tests_run += 1
                  
                  try:
                      # Load pipeline and example data
                      pipeline_data = load_and_fix_pipeline(pipeline_file)
                      example_data = load_json_file(example_file)
                      expected_result = load_json_file(results_file)
                      
                      # Run simulation
                      actual_result = simulate_pipeline(pipeline_data, example_data['docs'])
                      
                      # Normalize results for comparison (remove timestamps)
                      normalized_actual = normalize_result(actual_result)
                      normalized_expected = normalize_result(expected_result)
                      
                      # Compare results
                      if normalized_actual == normalized_expected:
                          print("✅ [SUCCESS] Simulation result matches expected result.")
                          tests_passed += 1
                      else:
                          print("❌ [FAILURE] Simulation result does not match expected result.")
                          print("---------- EXPECTED ----------")
                          print(json.dumps(normalized_expected, indent=2, sort_keys=True))
                          print("----------- ACTUAL -----------")
                          print(json.dumps(normalized_actual, indent=2, sort_keys=True))
                          print("----------------------------")
                          tests_failed += 1
                  
                  except Exception as e:
                      print(f"❌ [ERROR] {e}")
                      tests_failed += 1
              
              print()
              print("📊 Test Summary:")
              print(f"   Tests run: {tests_run}")
              print(f"   Passed: {tests_passed}")
              print(f"   Failed: {tests_failed}")
              print()
              
              if tests_failed > 0:
                  print("❌ One or more pipeline tests failed.")
                  return 1
              else:
                  print("✅ All pipeline tests passed!")
                  return 0

          if __name__ == "__main__":
              sys.exit(main())
          PYTHON_SCRIPT_EOF
          
          # Set environment variables from previous step
          export PIPELINES_TO_TEST="${{ steps.changed-pipelines.outputs.pipelines_to_test }}"
          export TEST_COUNT="${{ steps.changed-pipelines.outputs.test_count }}"
          
          # Run the Python validation script
          python3 validate_pipelines.py
